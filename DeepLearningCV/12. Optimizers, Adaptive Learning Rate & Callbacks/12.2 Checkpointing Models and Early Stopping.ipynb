{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Number of Classes: 10\n",
      "WARNING:tensorflow:From /Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "# loads the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "\n",
    "# Lets store the number of rows and columns\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "\n",
    "# Getting our date in the right 'shape' needed for Keras\n",
    "# We need to add a 4th dimenion to our date thereby changing our\n",
    "# Our original image shape of (60000,28,28) to (60000,28,28,1)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# store the shape of a single image \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# change our image type to float32 data type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Now we one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Let's count the number columns in our hot encoded matrix \n",
    "print (\"Number of Classes: \" + str(y_test.shape[1]))\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"/Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/loctv/miniconda3/envs/cv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      " - 110s - loss: 0.2276 - acc: 0.9293 - val_loss: 0.0752 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07520, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 2/15\n",
      " - 112s - loss: 0.0821 - acc: 0.9766 - val_loss: 0.0389 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07520 to 0.03895, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 3/15\n",
      " - 125s - loss: 0.0607 - acc: 0.9819 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03895 to 0.03587, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 4/15\n",
      " - 142s - loss: 0.0503 - acc: 0.9852 - val_loss: 0.0363 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03587\n",
      "Epoch 5/15\n",
      " - 141s - loss: 0.0444 - acc: 0.9869 - val_loss: 0.0354 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03587 to 0.03544, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 6/15\n",
      " - 149s - loss: 0.0402 - acc: 0.9878 - val_loss: 0.0301 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03544 to 0.03008, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 7/15\n",
      " - 144s - loss: 0.0368 - acc: 0.9893 - val_loss: 0.0289 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03008 to 0.02890, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 8/15\n",
      " - 150s - loss: 0.0338 - acc: 0.9894 - val_loss: 0.0323 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02890\n",
      "Epoch 9/15\n",
      " - 142s - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0293 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02890\n",
      "Epoch 10/15\n",
      " - 127s - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0336 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02890\n",
      "Epoch 11/15\n",
      " - 116s - loss: 0.0301 - acc: 0.9908 - val_loss: 0.0290 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02890\n",
      "Epoch 12/15\n",
      " - 116s - loss: 0.0294 - acc: 0.9910 - val_loss: 0.0279 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02890 to 0.02790, saving model to /Users/loctv/Downloads/Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs/DeepLearningCV/Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 13/15\n",
      " - 115s - loss: 0.0275 - acc: 0.9916 - val_loss: 0.0309 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02790\n",
      "Epoch 14/15\n",
      " - 105s - loss: 0.0283 - acc: 0.9914 - val_loss: 0.0315 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02790\n",
      "Epoch 15/15\n",
      " - 107s - loss: 0.0279 - acc: 0.9916 - val_loss: 0.0309 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02790\n",
      "Test loss: 0.030947131898109364\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 2,\n",
    "          callbacks = callbacks,\n",
    "          validation_data = (x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Multiple Call Backs & Early Stopping\n",
    "\n",
    "We can use other call back methods to monitor our training process such as **Early Stopping**. Checkout the Keras documentation for more:\n",
    "- https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 3, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can attempt to run again to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " - 117s - loss: 0.0254 - acc: 0.9923 - val_loss: 0.0309 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.02790\n",
      "Epoch 2/3\n",
      " - 101s - loss: 0.0253 - acc: 0.9926 - val_loss: 0.0300 - val_acc: 0.9911\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.02790\n",
      "Epoch 3/3\n",
      " - 110s - loss: 0.0256 - acc: 0.9922 - val_loss: 0.0316 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02790\n",
      "Test loss: 0.03161914329738847\n",
      "Test accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=2,\n",
    "          callbacks = callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another useful callback is Reducing our learning Rate on Plateau\n",
    "\n",
    "We can avoid having our oscillate around the global minimum by attempting to reduce the Learn Rate by a certain fact. If no improvement is seen in our monitored metric (val_loss typically), we wait a certain number of epochs (patience) then this callback reduces the learning rate by a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
